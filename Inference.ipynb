{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4fa6d64-7655-4567-9b32-a59a2ebae87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image, Grayscale\n",
    "from torchvision.models import models\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92fdcc4d-3544-49e2-a74d-f8c17e195957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1af451-939e-43d5-9352-f6990a3ee538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=1,\n",
    "        depth=4,\n",
    "        wf=3,\n",
    "        padding=True,\n",
    "        batch_norm=False,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fcdff5c-e335-475b-86a2-d4802cff27b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2006/2006 [39:42<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# Regular expression for numerical sorting\n",
    "numbers = re.compile(r'(\\d+)')\n",
    "\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # Adjust size as needed\n",
    "        Grayscale(num_output_channels=1),  # Convert to grayscale with one channel\n",
    "        transforms.ToTensor(),\n",
    "        # Add other necessary transformations (normalization, etc.)\n",
    "    ])\n",
    "    image = Image.open(image_path)\n",
    "    return transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "# Load your trained model\n",
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load('VPAT_model_checkpoint_6h.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# List all image files in the directory\n",
    "df = pd.read_csv('dataset.csv')\n",
    "image_files = list(df['Path'])\n",
    "images_directory = r'D:\\CV_Project\\results\\integrals'\n",
    "image_files = sorted(glob.glob(os.path.join(images_directory, '*.png')), key=numericalSort)\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "batch_size = 16  # Change batch size according to memory constraints\n",
    "data_loader = DataLoader(image_files, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Output directory to save edited images\n",
    "output_directory = r'D:\\OneDrive - Johannes Kepler Universität Linz\\Artificial_Intelligence\\Computer_Vision\\predictions'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Define a counter for naming the outputs\n",
    "counter = 0\n",
    "\n",
    "# Loop through the DataLoader and process images using the model\n",
    "for batch in tqdm(data_loader):\n",
    "    input_images = torch.cat([preprocess_image(img_path) for img_path in batch])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_images = model(input_images)\n",
    "    \n",
    "    # Process and save each output image in the batch\n",
    "    for i, output_image in enumerate(output_images):\n",
    "        output_image = output_image.permute(1, 2, 0).cpu().detach().numpy()\n",
    "        output_image = np.clip(output_image, 0, 1)  # Clip values if necessary\n",
    "    \n",
    "        # Check image dimensions and adjust if needed\n",
    "        if output_image.shape[2] == 1:  # Convert single channel to 3 channels (grayscale to RGB)\n",
    "            output_image = np.repeat(output_image, 3, axis=2)\n",
    "        elif output_image.shape[2] != 3 and output_image.shape[2] != 4:\n",
    "            raise ValueError(\"Invalid image shape\")\n",
    "    \n",
    "        # Save the edited image\n",
    "        image_name = f\"output_{str(counter).zfill(5)}.png\"\n",
    "        image_path = os.path.join(output_directory, image_name)\n",
    "        plt.imsave(image_path, output_image)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5d79c-d8de-44ed-89c9-b617776388be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
